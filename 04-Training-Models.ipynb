{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models\n",
    "Up until this point, we've been working with machine learning models as if they were all black boxes, however there are many intricicies to how these models work and why certain models may be more effective than others in different situations. In this section we'll focus on some common regression models, in particular:\n",
    "* Linear Regression\n",
    "* Polynomial Regression\n",
    "* Logistic Regression\n",
    "* Softmax Regression\n",
    "\n",
    "## Linear Regression\n",
    "Generally, a linear model makes a prediction by simply computing a weighted sum of all of the input features, plus a constant called the *bias term* (or *intercept*): $$\\hat{y}=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...+\\theta_{n}x_{n}$$\n",
    "* $\\hat{y}$ is the *predicted value*\n",
    "* $n$ is the number of *features*\n",
    "* $x_{i}$ is the *$i^{th}$ feature*\n",
    "* $\\theta_{j}$ is the *$j^{th}$ model parameter* (including the bias term $\\theta_{0}$ and the feature weights $\\theta_{0}, \\theta_{1}, ..., \\theta_{n}$).\n",
    "\n",
    "This is a linear equation, but we can actual write this in what's knows as **vectorized form**:$$\\hat{y}=h_{\\bf \\theta}(\\textbf{x})={\\bf \\theta}\\bullet \\textbf{x}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
