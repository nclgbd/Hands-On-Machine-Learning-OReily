{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models\n",
    "Up until this point, we've been working with machine learning models as if they were all black boxes, however there are many intricicies to how these models work and why certain models may be more effective than others in different situations. In this section we'll focus on some common regression models, in particular:\n",
    "* Linear Regression\n",
    "* Polynomial Regression\n",
    "* Logistic Regression\n",
    "* Softmax Regression\n",
    "\n",
    "## Linear Regression\n",
    "Generally, a linear model makes a prediction by simply computing a weighted sum of all of the input features, plus a constant called the *bias term* (or *intercept*): $$\\hat{y}=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...+\\theta_{n}x_{n}$$.\n",
    "* $\\hat{y}$ is the *predicted value*.\n",
    "* $n$ is the number of *features*.\n",
    "* $x_{i}$ is the *$i^{th}$ feature*.\n",
    "* $\\theta_{j}$ is the *$j^{th}$ model parameter* (including the bias term $\\theta_{0}$ and the feature weights $\\theta_{0}, \\theta_{1}, ..., \\theta_{n}$).\n",
    "\n",
    "This is a linear equation, but we can actual write this in what's knows as **vectorized form**:$$\\hat{y}=h_{\\bf \\theta}(\\textbf{x})={\\bf \\theta}\\bullet \\textbf{x}.$$\n",
    "* $\\bf{\\theta}$ is the model's *parameter vector*, containing the bias term $\\theta_{0}$ and the feature weights $\\theta_{1}$ to $\\theta_{n}$.\n",
    "* $\\textbf{x}$ is the instance's *feature vector*, containing $x_{0}$ to $x_{n}$, with $x_{0}$ always equal to 1.\n",
    "* $\\bf{\\theta \\bullet x}$ is the dot product of the vectors $\\bf{\\theta}$ and $\\bf{x}$, which is of course equal to $\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...+\\theta_{n}x_{n}$.\n",
    "* $h_{\\theta}$ is the hypothesis function, using the model parameters $\\bf{\\theta}$.\n",
    "\n",
    "In Machine Learning, vectors are often represented as *column vectors*, which are 2D arrays with a single column. If $\\bf{\\theta}$ and $\\bf{x}$ are column vectors, then the prediction is $$\\hat{y}=\\bf{\\theta^{\\bf{T}}x}$$ where $\\bf{\\theta^{\\bf{T}}}$ is the *transpose* of $\\bf{\\theta}$ (a row vector instead of a column vector) and $\\bf{\\theta^{\\bf{T}}x}$ is the matrix multiplication is $\\bf{\\theta^{\\bf{T}}}$ and $\\bf{x}$. It is of course the same prediction, except it is now represented as a single cell matrix rather than a scalar value.\n",
    "\n",
    "The most common performance measure of a regression model is the **Root Mean Square Error (RSME).** Therefore, to train a linear regression model, you need to find the value of $\\bf{\\theta}$ thatminimizes the RSME. Ib practice, it is simpler to minimize the **Mean Square Error (MSE)** than the RMSE.\n",
    "\n",
    "This is calculated using the following equation: $$MSE(\\bf{X}, h_{\\theta})=\\frac{1}{m}\\sum_{i=1}^{m}(\\bf{\\theta^{T}x^{(i)}-y^{(i)}})^{2}$$\n",
    "\n",
    "### The Normal Equation\n",
    "To find this $\\bf{\\theta}$, there is a closed-form solution known as the **normal equation.**: $$\\hat{\\bf{\\theta}}=(\\bf{X^{T}}X)^{-1}\\bf{X^{T}}y$$\n",
    "* $\\hat{\\theta}$ ia the value of $\\bf{\\theta}$ that minimizes the cost function.\n",
    "* $y$ is the vector of target values containing $y^{1}$ to $y^{(m)}$\n",
    "\n",
    "Let's create some linear-looking data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3*X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find $\\hat{\\theta}$ using the `inv()` function from the linear algebra module in NumPy (`np.linalg`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.02598307],\n",
       "       [3.10387674]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) # compute the dot product\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual function is $y=4+3x_{1}+$ Gaussian noise, so best case scenerio $\\theta_{0}=4$ and $\\theta_{1}=3$. Using the normal equation, we've instead calculated $\\theta_{0}=3.96033848$ and $\\theta_{1}=3.23984268$. \n",
    "\n",
    "From here we can make a linear prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.02598307],\n",
       "       [10.23373655]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot this to get a visual representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20a048afef0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(X_new, y_predict, 'r-')\n",
    "plt.plot(X, y, 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially what the `LinearRegression` class does, and we can run it as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.02598307]), array([[3.10387674]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "lr.intercept_, lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.02598307],\n",
       "       [3.10387674]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "*Gradient Descent* is a common algorithm used to find the optimal solution to a wide variety of problems. The general idea is to make gradual tweaks to your parameters in order to minimize the cost function. Since the cost function for many linear regression (and other) models is parabolic, this means that we can find a minimum of the function. Since this function is unknown, however, we used a **learning parameter** to learn where on the curve we are, and once we've hit the bottom we're complete!\n",
    "\n",
    "It's clear to see that the learning parameter is an important part of using gradient descent, as a learning parameter that is too small will take many iterations to find the minimum, while a learning parameter that is too high will overcompensate and thus still take too many iterations.\n",
    "\n",
    "Since MSE is a *convex* and *continouos function* with a slope that never abruptly changes, using gradient descent is almost guaranteed to get us close to our global minimum.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "To calculate the gradient descent, you use this formula:$$\\frac{\\partial}{\\partial \\theta_{j}}MSE(\\theta)=\\frac{2}{m}\\sum_{i=1}^{m}(\\theta^{T}x^{(i)}-y^{(i)})x_{j}^{(i)}.$$\n",
    "There's also a closed formula for calculating the gradient descent as well, denoted as $\\nabla_{\\theta}MSE(\\theta)-\\theta$. To calculate the next step, we use the learning step (denoted as $\\eta$):$$\\theta^{(next step)}=\\theta-\\eta\\nabla_{\\theta}MSE(\\theta).$$\n",
    "We can see this implemented with code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.02598307],\n",
       "       [3.10387674]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1)\n",
    "count = 0\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = (2/m)*X_b.T.dot(X_b.dot(theta)-y)\n",
    "    theta = theta-eta*gradients\n",
    "    count += 1\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that we found it! However let's try with different learning rates we may not be able to reach the minimum. Another thing to consider is how to select how many iterations we do, as too few iterations and we may not find our global minimum, and too many iterations and it will take too long. A way around this is to pick some sort of *tolerance* value ($\\epsilon$) of which to stop searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
